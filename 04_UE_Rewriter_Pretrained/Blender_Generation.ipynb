{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aa4e03b",
   "metadata": {},
   "source": [
    "### 0. data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "201008bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read all data\n",
    "docs=[]\n",
    "import json\n",
    "f = open('../wizard_of_wikipedia/data.json')\n",
    "data = json.load(f)\n",
    "  \n",
    "for i in data:\n",
    "    docs.append(i)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef02324f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "for doc_num in range(2):\n",
    "    dialog = []\n",
    "    for i in docs[doc_num]['dialog']:\n",
    "        dialog.append(i['text'])\n",
    "    dialog_lower = [text.lower() for text in dialog]\n",
    "    all_data.append(dialog_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3689194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i'm a huge fan of science fiction myself! \""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024e08c5",
   "metadata": {},
   "source": [
    "### 1. blenderbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62e798b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08faa790c77e471f845243f7252f4bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "801701a446304f42b584120515a2c2e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/641 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db54723c8be34a38aa5046d91b63868e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43d29b77c8954e94ad72b6a7fbdb4743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50672a71bf624dfa8eabd1d886f00b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/335M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c61c4b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = tokenizer(all_data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61b6ce94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72, 1101, 257, 3236, 4336, 286, 3783, 10165, 3589, 0, 220]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f571c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(torch.tensor(sen['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1eb052",
   "metadata": {},
   "source": [
    "### 2. pred_results (saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "650ad580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 50257])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.logits.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b949b4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[   72,  1101,   257,  3236,  4336,   286,  3783, 10165,  3589,     0,\n",
       "           220,   425,  1100,   262,   717,   734,   837,   475,  4398,   470]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(torch.tensor([sen['input_ids']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "12763334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: ['awesome! i really love how sci-fi storytellers focus on political/social/philosophical issues that would still be around even in the future. makes them relatable.to the future, and makes them more relatable to people who are still living in the future!!!?!?!?!?!?!?!?!?!?!?!?!?!?!?!?!?!?!?!?!?!?!?!?!?!?!?!?!?!?!?!?!?!?!?!']\n"
     ]
    }
   ],
   "source": [
    "document = (all_data[0][2])\n",
    "# encode input context\n",
    "input_ids = tokenizer(document, return_tensors=\"pt\").input_ids\n",
    "# generate 3 independent sequences using beam search decoding (5 beams)\n",
    "# with T5 encoder-decoder model conditioned on short news article.\n",
    "outputs = model.generate(input_ids=input_ids, num_beams=5, num_return_sequences=1, min_length=100, max_length=200)\n",
    "print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6bba0373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'awesome! i really love how sci-fi storytellers focus on political/social/philosophical issues that would still be around even in the future. makes them relatable.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f0fa943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot_small-90M\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/blenderbot_small-90M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "24cb5cc8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: [\"i agree, it's a great way to get to know your audience. i'm glad you like it, i've been reading a lot of sci - fi lately, and i think it has a lot to do with the way we interact with each other, how we interact, and how we relate to each other. i think that's one of the most important things to do in life, and it makes it so much easier to relate to people when you're trying to understand them.\"]\n"
     ]
    }
   ],
   "source": [
    "document = (all_data[0][2])\n",
    "# encode input context\n",
    "input_ids = tokenizer(document, return_tensors=\"pt\").input_ids\n",
    "# generate 3 independent sequences using beam search decoding (5 beams)\n",
    "# with T5 encoder-decoder model conditioned on short news article.\n",
    "outputs = model.generate(input_ids=input_ids, num_beams=5, num_return_sequences=1, min_length=100, max_length=200)\n",
    "print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fd890cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"i think science fiction is an amazing genre for anything. future science, technology, time travel, ftl travel, they're all such interesting concepts.\",\n",
       "  \"i'm a huge fan of science fiction myself! \",\n",
       "  'awesome! i really love how sci-fi storytellers focus on political/social/philosophical issues that would still be around even in the future. makes them relatable.',\n",
       "  'i agree. one of my favorite forms of science fiction is anything related to time travel! i find it fascinating.',\n",
       "  \"it's not quite sci-fi, but my favorite version of time travel is in harry potter and the prisoner of azkaban. breaks zero logical rules.\",\n",
       "  \"and that's difficult to do when dealing with time travel. i actually haven't seen the latest harry potter movies. guess it's time to check them out!\",\n",
       "  'if you really want a look at the potential negative consequences of scientific innovation, what you should check out is the tv show fringe. incredibly well written.',\n",
       "  'thank you for the suggestion, i will definitely check it out!',\n",
       "  \"it blends science fiction and paranormal/psychological/mk ultra type stuff together, but it's science fiction at its core.\",\n",
       "  'always looking for more science fiction to digest!'],\n",
       " ['can you imagine the world without internet access? ',\n",
       "  \"no i could not! i couldn't imagine living when internet access was rare and very few people had it!\",\n",
       "  'oh me either! it seems like such a long time ago. i wonder when internet was first created?',\n",
       "  'it used to be restricted, but around 1995, the restricted were lifted and commercial use of it began',\n",
       "  'that is awesome. i wonder why it was restricted? probably because they only wanted government and big companies to use it at first.',\n",
       "  'yes, it was developed from a government funded projects to help with universities research and laboratories in the united states...i am so glad they expanded it! ',\n",
       "  'i am too, it makes life so much easier!',\n",
       "  'what is your favorite thing to do with internet access? i like being able to use my computer and smartphone to use my email and browse the world wide web']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ab1dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict = all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6309acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in [\n",
    "            to_predict[i : i + self.args.eval_batch_size] for i in range(0, len(to_predict), self.args.eval_batch_size)\n",
    "        ]:\n",
    "            if self.args.model_type == \"marian\":\n",
    "                input_ids = self.encoder_tokenizer.prepare_translation_batch(\n",
    "                    batch, max_length=self.args.max_seq_length, padding='max_length', truncation=True, return_tensors=\"pt\",\n",
    "                )[\"input_ids\"]\n",
    "            else:\n",
    "                input_ids = self.encoder_tokenizer.batch_encode_plus(\n",
    "                    batch, max_length=self.args.max_seq_length, padding='max_length', truncation=True, return_tensors=\"pt\",\n",
    "                )[\"input_ids\"]\n",
    "            input_ids = input_ids.to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13990bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, to_predict):\n",
    "        \"\"\"\n",
    "        Performs predictions on a list of text.\n",
    "\n",
    "        Args:\n",
    "            to_predict: A python list of text (str) to be sent to the model for prediction. Note that the prefix should be prepended to the text.\n",
    "\n",
    "        Returns:\n",
    "            preds: A python list of the generated sequences.\n",
    "        \"\"\"  # noqa: ignore flake8\"\n",
    "\n",
    "        self._move_model_to_device()\n",
    "\n",
    "        all_outputs = []\n",
    "        # Batching\n",
    "        for batch in [\n",
    "            to_predict[i : i + self.args.eval_batch_size] for i in range(0, len(to_predict), self.args.eval_batch_size)\n",
    "        ]:\n",
    "            if self.args.model_type == \"marian\":\n",
    "                input_ids = self.encoder_tokenizer.prepare_translation_batch(\n",
    "                    batch, max_length=self.args.max_seq_length, padding='max_length', truncation=True, return_tensors=\"pt\",\n",
    "                )[\"input_ids\"]\n",
    "            else:\n",
    "                input_ids = self.encoder_tokenizer.batch_encode_plus(\n",
    "                    batch, max_length=self.args.max_seq_length, padding='max_length', truncation=True, return_tensors=\"pt\",\n",
    "                )[\"input_ids\"]\n",
    "            input_ids = input_ids.to(self.device)\n",
    "\n",
    "            if self.args.model_type in [\"bart\", \"marian\", \"blender\", \"blender-large\"]:\n",
    "\n",
    "#changed 'generate'\n",
    "                outputs = self.model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    num_beams=self.args.num_beams,\n",
    "                    max_length=self.args.max_length,\n",
    "                    length_penalty=self.args.length_penalty,\n",
    "                    early_stopping=self.args.early_stopping,\n",
    "                    repetition_penalty=self.args.repetition_penalty,\n",
    "                    do_sample=self.args.do_sample,\n",
    "                    top_k=self.args.top_k,\n",
    "                    top_p=self.args.top_p,\n",
    "                    num_return_sequences=self.args.num_return_sequences,\n",
    "                    # temperature=0.7\n",
    "                )\n",
    "            else:\n",
    "                outputs = self.model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    decoder_start_token_id=self.model.config.decoder.pad_token_id,\n",
    "                    num_beams=self.args.num_beams,\n",
    "                    max_length=self.args.max_length,\n",
    "                    length_penalty=self.args.length_penalty,\n",
    "                    early_stopping=self.args.early_stopping,\n",
    "                    repetition_penalty=self.args.repetition_penalty,\n",
    "                    do_sample=self.args.do_sample,\n",
    "                    top_k=self.args.top_k,\n",
    "                    top_p=self.args.top_p,\n",
    "                    num_return_sequences=self.args.num_return_sequences,\n",
    "                )\n",
    "\n",
    "            all_outputs.extend(outputs.cpu().numpy())\n",
    "\n",
    "        if self.args.use_multiprocessed_decoding:\n",
    "            self.model.to(\"cpu\")\n",
    "            with Pool(self.args.process_count) as p:\n",
    "                outputs = list(\n",
    "                    tqdm(\n",
    "                        p.imap(self._decode, all_outputs, chunksize=self.args.multiprocessing_chunksize),\n",
    "                        total=len(all_outputs),\n",
    "                        desc=\"Decoding outputs\",\n",
    "                        disable=self.args.silent,\n",
    "                    )\n",
    "                )\n",
    "            self._move_model_to_device()\n",
    "        else:\n",
    "            outputs = [\n",
    "                self.decoder_tokenizer.decode(output_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "                for output_id in all_outputs\n",
    "            ]\n",
    "\n",
    "        if self.args.num_return_sequences > 1:\n",
    "            return [\n",
    "                outputs[i : i + self.args.num_return_sequences]\n",
    "                for i in range(0, len(outputs), self.args.num_return_sequences)\n",
    "            ]\n",
    "        else:\n",
    "            return outputs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
