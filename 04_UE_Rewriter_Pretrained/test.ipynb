{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26a97619",
   "metadata": {},
   "source": [
    "以下部分为测试UE Rewriter在pretrained dialogue system上测试提供数据。rewriter_data.txt为将所有unseen entity 经UE Rewriter替换成了seen entity后形成的“新”数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9937db3",
   "metadata": {},
   "source": [
    "### 0. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f37bae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read all data\n",
    "docs=[]\n",
    "import json\n",
    "f = open('../wizard_of_wikipedia/data.json')\n",
    "data = json.load(f)\n",
    "  \n",
    "for i in data:\n",
    "    docs.append(i)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13a69148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22311"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd6c5d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "990b06d6",
   "metadata": {},
   "source": [
    "### 1. whether inputs contain unseen entities\n",
    "- 定位id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d255ba67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ms/fldvpzcn3h5dttydk57kxqv40000gn/T/ipykernel_5807/931073343.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mdialog_lower\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdialog\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mall_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdialog_lower\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "all_data = []\n",
    "unseen_dataset = pd.DataFrame()\n",
    "sentences=[]\n",
    "unseen_entities=[]\n",
    "doc_nums=[]\n",
    "dialog_indices=[]\n",
    "\n",
    "\n",
    "for doc_num in tqdm(range(100,102)):\n",
    "    dialog = []\n",
    "    for i in docs[doc_num]['dialog']:\n",
    "        dialog.append(i['text'])\n",
    "    dialog_lower = [text.lower() for text in dialog]\n",
    "    all_data.append(dialog_lower)\n",
    "    print(all_data[1][7])\n",
    "    break\n",
    "    \n",
    "    #build vocabulary\n",
    "    text = ''.join(dialog)\n",
    "    clean_text = re.sub(r\"[,.;@#?!&$/]+\\ *\", \" \", text)\n",
    "    vocabulary = set(clean_text.lower().split())\n",
    "    \n",
    "    #BERT tokenization\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    indexed_tokens = []\n",
    "    for text in dialog:   \n",
    "        tokenized_text = tokenizer.tokenize(text)\n",
    "        indexed_tokens.append(tokenizer.convert_tokens_to_ids(tokenized_text))\n",
    "    \n",
    "    #BERT tokenized vocabulary\n",
    "    compact = []\n",
    "    for i in indexed_tokens:\n",
    "        compact.extend(i) \n",
    "    tokenized_vocab = set(compact)\n",
    "    \n",
    "    #BERT text vocabulary\n",
    "    new_vocab = set(tokenizer.convert_ids_to_tokens(tokenized_vocab))\n",
    "    \n",
    "    #unseen words in BERT\n",
    "    unseen = vocabulary.difference(new_vocab)\n",
    "    unseen = list(unseen)\n",
    "\n",
    "    #find sentences with unseen entities\n",
    "    for word in unseen:\n",
    "        indices = [i for i, x in enumerate([word in i for i in dialog_lower]) if x == True] \n",
    "        for index in indices:\n",
    "            sentence = dialog_lower[index]\n",
    "            result = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "            result = dict(result)\n",
    "            if word in result:\n",
    "                if result[word] in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "                    sentences.append(dialog[index])\n",
    "                    unseen_entities.append(word)\n",
    "                    doc_nums.append(doc_num)\n",
    "                    dialog_indices.append(index)\n",
    "\n",
    "unseen_dataset['unseen entity'] = unseen_entities\n",
    "#unseen_dataset['sentence'] = sentences\n",
    "unseen_dataset['doc number'] = doc_nums\n",
    "unseen_dataset['dialog index'] = dialog_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdd29bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unseen entity</th>\n",
       "      <th>doc number</th>\n",
       "      <th>dialog index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gluten</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>greatness</td>\n",
       "      <td>101</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thats</td>\n",
       "      <td>101</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>prostheses</td>\n",
       "      <td>101</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lol</td>\n",
       "      <td>101</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  unseen entity  doc number  dialog index\n",
       "0        gluten         100             5\n",
       "1     greatness         101             7\n",
       "2         thats         101             2\n",
       "3    prostheses         101             7\n",
       "4           lol         101             5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unseen_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57191a20",
   "metadata": {},
   "source": [
    "### 2. UE Rewriter\n",
    "- window_size作为参数，用id实现\n",
    "- 注意window size是re-writer的参数，而不是放入生成模型的参数\n",
    "- mask预测的模型，目前只选取了概率最高的，是否要做多个实验需要考虑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7c224c6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "# model.to('cuda')  # if you have gpu\n",
    "\n",
    "\n",
    "def predict_masked_sent(text, top_k=5):\n",
    "    # Tokenize input\n",
    "    text = \"[CLS] %s [SEP]\"%text\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    masked_index = tokenized_text.index(\"[MASK]\")\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    # tokens_tensor = tokens_tensor.to('cuda')    # if you have gpu\n",
    "\n",
    "    # Predict all tokens\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor)\n",
    "        predictions = outputs[0]\n",
    "\n",
    "    probs = torch.nn.functional.softmax(predictions[0, masked_index], dim=-1)\n",
    "    top_k_weights, top_k_indices = torch.topk(probs, top_k, sorted=True)\n",
    "    \n",
    "    pred_with_prob = {}\n",
    "    for i, pred_idx in enumerate(top_k_indices):\n",
    "        predicted_token = tokenizer.convert_ids_to_tokens([pred_idx])[0]\n",
    "        token_weight = top_k_weights[i]\n",
    "        pred_with_prob[predicted_token] = float(token_weight)\n",
    "        #print(\"[MASK]: '%s'\"%predicted_token, \" | weights:\", float(token_weight))\n",
    "    return pred_with_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef4de855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 5/5 [00:01<00:00,  4.36it/s]\n"
     ]
    }
   ],
   "source": [
    "#Replace all unseen entities with [MASK] and put the masked sentences into all_data\n",
    "window_size = 0\n",
    "ex = [] #long sentence cannot be tokenized\n",
    "\n",
    "for i in tqdm(range(len(unseen_dataset))):\n",
    "    doc_num = unseen_dataset['doc number'][i]-100\n",
    "    dialog_num = unseen_dataset['dialog index'][i]\n",
    "    unseen_entity = unseen_dataset['unseen entity'][i]\n",
    "    \n",
    "    unseen_sentence = all_data[doc_num][dialog_num]\n",
    "    mask_sentence = unseen_sentence.replace(unseen_entity, \"[MASK]\")\n",
    "    \n",
    "    if window_size==0 or dialog_num < window_size:\n",
    "        try:\n",
    "            pred = predict_masked_sent(mask_sentence, top_k=1)\n",
    "            UE_pred = list(pred.keys())[0]\n",
    "        #except ValueError:\n",
    "            #UE_pred = unseen_entity\n",
    "        except RuntimeError:\n",
    "            ex.append((doc_num, dialog_num))\n",
    "    \n",
    "    else:\n",
    "        context = \"\"\n",
    "        for sen in all_data[doc_num][dialog_num-window_size : dialog_num]:\n",
    "            context = context+sen\n",
    "        mask_sentence = context+mask_sentence\n",
    "        try:\n",
    "            pred = predict_masked_sent(mask_sentence, top_k=1)\n",
    "            UE_pred = list(pred.keys())[0]\n",
    "        except ValueError:\n",
    "            UE_pred = unseen_entity\n",
    "        except RuntimeError:\n",
    "            ex.append((doc_num, dialog_num))\n",
    "    \n",
    "    rewrited_sentence = mask_sentence.replace(\"[MASK]\", UE_pred)\n",
    "    del all_data[doc_num][dialog_num]\n",
    "    all_data[doc_num].insert(dialog_num, rewrited_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54edffdf",
   "metadata": {},
   "source": [
    "将经过UE-Rewriter的数据保存成txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdd1251",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('./rewrite_data_w0.txt','a')\n",
    "for dialog in all_data:\n",
    "    for sen in dialog:\n",
    "        file.write(sen)\n",
    "        file.write('\\n')\n",
    "    file.write('##')    \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bc9cce",
   "metadata": {},
   "source": [
    "### 3. To read the new txt\n",
    "- 不同window size rewrite之后，保存下来，再放pretrain，以免重复工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23929f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('./rewrite_data_w0/rewrite_data_w0_A.txt','r')\n",
    "file_data = file.read() \n",
    "file_data = file_data.split('##')\n",
    "\n",
    "while \"\" in file_data:\n",
    "    file_data.remove(\"\")\n",
    "    \n",
    "data = []\n",
    "for dialog in file_data:\n",
    "    tep_list = dialog.split('\\n')\n",
    "    del(tep_list[-1])\n",
    "    data.append(tep_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1bf952b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4432"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0297116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_duplicates(t):\n",
    "    for i in t:\n",
    "        if i in t.pop(t.index(i)):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e40abae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "has_duplicates(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22c4b1ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sh! that was a super tough game. i am really looking forward to student tickets to all the home games!',\n",
       " 'i live in dallas tx now so i only get to watch on tv.  two years ago alabama played usc out here in arlington, tx and i was able to go.  i hope you enjoy your senior year.  you only get to do it once so make the most of it.',\n",
       " 'thank you. i am going to try.  my family will make sure of it, too - always. ',\n",
       " 'i am sure you will have a blast.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0218aad1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
